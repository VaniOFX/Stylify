{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../../')\n",
    "from fastai.conv_learner import *\n",
    "from fastai.dataset import *\n",
    "from pathlib import Path\n",
    "from PIL import ImageDraw, ImageFont\n",
    "from matplotlib import patches, patheffects\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_model=resnet50\n",
    "size=224\n",
    "export = \"../models/modanet-ref-resnet50.pkl\"\n",
    "\n",
    "aug_tfms = [RandomRotate(10, p=0.5, tfm_y=TfmType.COORD),\n",
    "            RandomLighting(0.05, 0.05, tfm_y=TfmType.COORD),\n",
    "            RandomFlip(tfm_y=TfmType.COORD)]\n",
    "\n",
    "tfms = tfms_from_model(f_model,\n",
    "                       size,\n",
    "                       crop_type=CropType.NO,\n",
    "                       tfm_y=TfmType.COORD,\n",
    "                       aug_tfms=aug_tfms)\n",
    "\n",
    "anchor_grid_sizes = [28,14,7,4,2,1]\n",
    "anchor_zooms =  [.7, 2**0, 2**(1/3), 2**(2/3)]\n",
    "anchor_ratios = [(1.,1.), (.5,1.), (1.,.5), (3.,1.), (1.,3.)]\n",
    "\n",
    "anchor_scales = [(zoom*n, zoom*m) for zoom in anchor_zooms for (n, m) in anchor_ratios]\n",
    "k = len(anchor_scales)\n",
    "\n",
    "# the center offsets\n",
    "anchor_offsets = [1/(size*2) for size in anchor_grid_sizes]\n",
    "\n",
    "# create the x coordinates for the anchors\n",
    "anchor_xs = np.concatenate([np.repeat(np.linspace(offset, 1-offset, gsize), gsize)\n",
    "                            for offset, gsize in zip(anchor_offsets, anchor_grid_sizes)])\n",
    "\n",
    "# create the y coordinates for the anchors\n",
    "anchor_ys = np.concatenate([np.tile(np.linspace(offset, 1-offset, gsize), gsize)\n",
    "                            for offset, gsize in zip(anchor_offsets, anchor_grid_sizes)])\n",
    "\n",
    "# create k anchor boxes per grid cell, all with the  same center coordinates\n",
    "anchor_centres = np.repeat(np.stack([anchor_xs, anchor_ys], axis=1), k, axis=0)\n",
    "\n",
    "# create the height and width of the anchors\n",
    "anchor_sizes = np.concatenate([np.array([[h/gsize, w/gsize] for i in range(gsize*gsize) for h,w in anchor_scales])\n",
    "                            for gsize in anchor_grid_sizes])\n",
    "\n",
    "# create a pytorch variable with the size of the grid cell for each default anchor box\n",
    "grid_sizes = V(np.concatenate([np.array([1/gsize for i in range(gsize*gsize) for h,w in anchor_scales])\n",
    "               for gsize in anchor_grid_sizes]), requires_grad=False).unsqueeze(1)\n",
    "\n",
    "# create the anchor coordinates (x_center, y_center, height, width)\n",
    "anchor_hws = V(np.concatenate([anchor_centres, anchor_sizes], axis=1), requires_grad=False).float()\n",
    "\n",
    "\n",
    "def conv_layer(num_in, num_out, stride=2, dp= 0.1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(num_in, num_out, 3, bias=False, stride=stride, padding=1),\n",
    "        nn.BatchNorm2d(num_out, momentum=0.01),\n",
    "        nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
    "        nn.Dropout(dp))\n",
    "    \n",
    "class OutputConvolution(nn.Module):\n",
    "    def __init__(self, k, num_in, bias):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cat_output = nn.Conv2d(num_in, (len(id2cat)+1) * k, 3, padding=1)\n",
    "        self.cat_output.bias.data.zero_().add_(bias)\n",
    "\n",
    "        self.bbox_output = nn.Conv2d(num_in, 4 * k, 3, padding = 1)\n",
    "        self.k = k\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return [flatten(self.cat_output(x), self.k),\n",
    "                flatten(self.bbox_output(x), self.k)]\n",
    "    \n",
    "def flatten(x, k):\n",
    "    batch_size, num_filters, _, _ = x.size()\n",
    "    x = x.permute(0,2,3,1).contiguous()\n",
    "    return x.view(batch_size, -1, num_filters//k)\n",
    "\n",
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): \n",
    "        self.features = output\n",
    "        self.features_in = input\n",
    "    def remove(self): self.hook.remove()\n",
    "        \n",
    "drop = 0.4\n",
    "\n",
    "class FPN_SSD_Model(nn.Module):\n",
    "    def __init__(self, model_base, k, bias):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_base = model_base\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "        # The convolutional layers that reduce the grid size\n",
    "        # that form the top-down pathway (28x28, 14x14, 7x7, 4x4, 2x2, 1x1)\n",
    "        self.saved_features = [SaveFeatures(model_base[i]) for i in [5,6]]\n",
    "        self.conv7 = conv_layer(2048,1024, dp=drop, stride=1) \n",
    "        self.conv4 = conv_layer(1024,512, dp=drop) \n",
    "        self.conv2 = conv_layer(512,512, dp=drop) \n",
    "        self.conv1 = conv_layer(512,512, dp=drop) \n",
    "        \n",
    "        # Layers for forming the lateral connections (28x28, 14x14, 7x7, 4x4, 2x2, 1x1)\n",
    "        self.lat28 = nn.Conv2d(512,512,kernel_size=1, stride=1, padding=0)\n",
    "        self.lat14 = nn.Conv2d(1024,512,kernel_size=1, stride=1, padding=0)\n",
    "        self.lat7 = nn.Conv2d(1024,512,kernel_size=1, stride=1, padding=0)\n",
    "        self.lat4 = nn.Conv2d(512,512,kernel_size=1, stride=1, padding=0)\n",
    "        self.lat2 = nn.Conv2d(512,512,kernel_size=1, stride=1, padding=0)\n",
    "        self.lat1 = nn.Conv2d(512,512,kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        # The upsampling layers that increase the grid size\n",
    "        # that from the bottom-up pathway (2x2, 4x4, 7x7, 14x14, 28x28)\n",
    "        self.upsamp2 = nn.Upsample(size=(2,2), mode='bilinear')\n",
    "        self.upsamp4 = nn.Upsample(size=(4,4), mode='bilinear')\n",
    "        self.upsamp7 = nn.Upsample(size=(7,7), mode='bilinear')\n",
    "        self.upsamp14 = nn.Upsample(size=(14,14), mode='bilinear')\n",
    "        self.upsamp28 = nn.Upsample(size=(28,28), mode='bilinear')\n",
    "        \n",
    "        # The output convolutional layer to split the network\n",
    "        # for categories and bounding boxes (28x28, 14x14, 7x7, 4x4, 2x2, 1x1)\n",
    "        self.out28 = OutputConvolution(k, 512, bias)\n",
    "        self.out14 = OutputConvolution(k, 512, bias)\n",
    "        self.out7 = OutputConvolution(k, 512, bias)\n",
    "        self.out4 = OutputConvolution(k, 512, bias)\n",
    "        self.out2 = OutputConvolution(k, 512, bias)\n",
    "        self.out1 = OutputConvolution(k, 512, bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # get the activations from the pre-trained model\n",
    "        x = self.drop(F.relu(self.model_base(x)))\n",
    "        \n",
    "        # get the activations from reducing the grid size\n",
    "        c28 = F.relu(self.saved_features[0].features)\n",
    "        c14 = F.relu(self.saved_features[1].features)\n",
    "        c7 = self.conv7(x)\n",
    "        c4 = self.conv4(c7)\n",
    "        c2 = self.conv2(c4)\n",
    "        c1 = self.conv1(c2)\n",
    "       \n",
    "        # Upsampling and joining the lateral connections\n",
    "        p1 = self.lat1(c1)\n",
    "        p2 = self.upsamp2(p1) + self.lat2(c2)    \n",
    "        p4 = self.upsamp4(p2) + self.lat4(c4)\n",
    "        p7 = self.upsamp7(p4) + self.lat7(c7)\n",
    "        p14 = self.upsamp14(p7) + self.lat14(c14)\n",
    "        p28 = self.upsamp28(p14) + self.lat28(c28)\n",
    "        \n",
    "        # making the final predictions\n",
    "        out28cat,out28bbox = self.out28(p28)\n",
    "        out14cat,out14bbox = self.out14(p14)\n",
    "        out7cat,out7bbox = self.out7(p7)\n",
    "        out4cat,out4bbox = self.out4(p4)\n",
    "        out2cat,out2bbox = self.out2(p2)\n",
    "        out1cat,out1bbox = self.out1(p1)\n",
    "        \n",
    "        # concatenate all the predictions together\n",
    "        return [torch.cat([out28cat, out14cat, out7cat, out4cat, out2cat, out1cat], dim=1),\n",
    "                torch.cat([out28bbox, out14bbox, out7bbox, out4bbox, out2bbox, out1bbox], dim=1)]\n",
    "    \n",
    "    \n",
    "cut,lr_cut = model_meta[f_model]\n",
    "\n",
    "class MakeModel():\n",
    "    def __init__(self,model,name='makemodel'):\n",
    "        self.model,self.name = model,name\n",
    "\n",
    "    def get_layer_groups(self, precompute):\n",
    "        lgs = list(split_by_idxs(children(self.model.model_base), [lr_cut]))\n",
    "        return lgs + [children(self.model)[1:]]\n",
    "\n",
    "def get_base(f_model):\n",
    "    cut, _ = model_meta[f_model]\n",
    "    layers = cut_model(f_model(True), cut)\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class Fake():\n",
    "    def __init__(self,p):\n",
    "        self.path = p\n",
    "\n",
    "def hw2corners(ctr, hw): return torch.cat([ctr-hw/2, ctr+hw/2], dim=1)\n",
    "\n",
    "def preds_to_bbs(actn, anchor_hws):\n",
    "    # run the activations through a non-linear layer\n",
    "    actn_bbs = torch.tanh(actn)\n",
    "    # convert the first two activations to offsets in the centers of the default anchor boxes\n",
    "    actn_ctrs = torch.clamp(((actn_bbs[:,:2] * grid_sizes) + anchor_hws[:,:2]), 0, size)\n",
    "    # convert the the second two activations to scaled height and width of default anchor boxes\n",
    "    actn_hw = torch.clamp(((1 + actn_bbs[:,2:]) * anchor_hws[:,2:]), 0, size)\n",
    "    # convert the bounding boxes from (center_x, center_y, height, widht) -> (x1,x2,y1,y2)\n",
    "    return hw2corners(actn_ctrs, actn_hw)\n",
    "\n",
    "def nms(boxes, scores, overlap=0.5, top_k=100):\n",
    "    keep = scores.new(scores.size(0)).zero_().long()\n",
    "    if boxes.numel() == 0: return keep\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    area = torch.mul(x2 - x1, y2 - y1)\n",
    "    v, idx = scores.sort(0)  # sort in ascending order\n",
    "    idx = idx[-top_k:]  # indices of the top-k largest vals\n",
    "    xx1 = boxes.new()\n",
    "    yy1 = boxes.new()\n",
    "    xx2 = boxes.new()\n",
    "    yy2 = boxes.new()\n",
    "    w = boxes.new()\n",
    "    h = boxes.new()\n",
    "\n",
    "    count = 0\n",
    "    while idx.numel() > 0:\n",
    "        i = idx[-1]  # index of current largest val\n",
    "        keep[count] = i\n",
    "        count += 1\n",
    "        if idx.size(0) == 1: break\n",
    "        idx = idx[:-1]  # remove kept element from view\n",
    "        # load bboxes of next highest vals\n",
    "        torch.index_select(x1, 0, idx, out=xx1)\n",
    "        torch.index_select(y1, 0, idx, out=yy1)\n",
    "        torch.index_select(x2, 0, idx, out=xx2)\n",
    "        torch.index_select(y2, 0, idx, out=yy2)\n",
    "        # store element-wise max with next highest score\n",
    "        xx1 = torch.clamp(xx1, min=x1[i])\n",
    "        yy1 = torch.clamp(yy1, min=y1[i])\n",
    "        xx2 = torch.clamp(xx2, max=x2[i])\n",
    "        yy2 = torch.clamp(yy2, max=y2[i])\n",
    "        w.resize_as_(xx2)\n",
    "        h.resize_as_(yy2)\n",
    "        w = xx2 - xx1\n",
    "        h = yy2 - yy1\n",
    "        # check sizes of xx1 and xx2.. after each iteration\n",
    "        w = torch.clamp(w, min=0.0)\n",
    "        h = torch.clamp(h, min=0.0)\n",
    "        inter = w*h\n",
    "        # IoU = i / (area(a) + area(b) - i)\n",
    "        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n",
    "        union = (rem_areas - inter) + area[i]\n",
    "        IoU = inter/union  # store result in iou\n",
    "        # keep only elements with an IoU <= overlap\n",
    "        idx = idx[IoU.le(overlap)]\n",
    "    return keep, count\n",
    "\n",
    "def load_learner():\n",
    "    state = torch.load(export)\n",
    "    id2cat = state.pop(\"classes\")\n",
    "    cat2id = {c:i for i,c in enumerate(id2cat)}\n",
    "    p = state.pop(\"path\")\n",
    "    mm = state.pop(\"model\")\n",
    "    c = state.pop(\"class\")\n",
    "    weights = state.pop(\"weights\")\n",
    "    learn = c(Fake(p), mm)\n",
    "    learn.load(weights)\n",
    "    return learn, id2cat, cat2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(learner, fn):\n",
    "    # open the image and run it through the transforms to pass to the model\n",
    "    image = open_image(fn)\n",
    "    image = tfms[1](image, np.zeros(4))[0][None]\n",
    "\n",
    "    # pass the image to the model\n",
    "    learner.model.cuda()\n",
    "    learner.model.eval()\n",
    "    pred_cat, pred_bb = learner.model(V(image))\n",
    "    \n",
    "    # get rid of the batch dimension\n",
    "    pred_cat.squeeze_()\n",
    "    pred_cat.sigmoid_()\n",
    "    pred_bb.squeeze_()\n",
    "    \n",
    "    # convert the activations to bounding boxes\n",
    "    pred_bb = preds_to_bbs(pred_bb, anchor_hws)\n",
    "    \n",
    "    # get the probability for each anchor boxes for each category\n",
    "    conf_scores = pred_cat.t().data\n",
    "    \n",
    "    final_scores, final_bbs, final_cats = [],[],[]\n",
    "    for cat in range(0, len(conf_scores)-1):\n",
    "        \n",
    "        # get only the anchor boxes with more than 25% probability\n",
    "        c_mask = conf_scores[cat] > 0.25\n",
    "        if c_mask.sum() == 0: continue\n",
    "            \n",
    "        # filter the confidence scores\n",
    "        scores = conf_scores[cat][c_mask]\n",
    "        \n",
    "        # filter the bounding boxes\n",
    "        l_mask = c_mask.unsqueeze(1).expand_as(pred_bb)\n",
    "        boxes = pred_bb[l_mask].view(-1, 4)\n",
    "        \n",
    "        # only keep one bounding box per item\n",
    "        ids, count = nms(boxes.data, scores, 0.4, 50)\n",
    "        ids = ids[:count]\n",
    "        \n",
    "        # append to the final results\n",
    "        final_scores.append(scores[ids])\n",
    "        final_bbs.append(boxes.data[ids])\n",
    "        final_cats.append([cat]*count)\n",
    "\n",
    "    if not final_cats:\n",
    "        return [], [], []\n",
    "    \n",
    "    final_cats = T(np.concatenate(final_cats))\n",
    "    final_scores = torch.cat(final_scores)\n",
    "    final_bbs = torch.cat(final_bbs)\n",
    "    \n",
    "    return final_bbs, final_cats, final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "  0.8872  0.4312  0.9816  0.5564\n",
       "  0.3452  0.3479  0.8688  0.6422\n",
       "  0.1819  0.3156  0.3391  0.6514\n",
       " [torch.cuda.FloatTensor of size 3x4 (GPU 0)], \n",
       "  3\n",
       "  7\n",
       "  8\n",
       " [torch.cuda.LongTensor of size 3 (GPU 0)], \n",
       "  0.2954\n",
       "  0.9845\n",
       "  0.7131\n",
       " [torch.cuda.FloatTensor of size 3 (GPU 0)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learner, _, _ = load_learner()\n",
    "# predict_fn(learner, \"../static/image_data/1571522_250538.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_model=resnet34\n",
    "# size=224\n",
    "\n",
    "# aug_tfms = [RandomRotate(10, tfm_y=TfmType.COORD),\n",
    "#             RandomLighting(0.05, 0.05, tfm_y=TfmType.COORD),\n",
    "#             RandomFlip(tfm_y=TfmType.COORD)]\n",
    "# tfms = tfms_from_model(f_model,\n",
    "#                        size,\n",
    "#                        crop_type=CropType.NO,\n",
    "#                        tfm_y=TfmType.COORD,\n",
    "#                        aug_tfms=aug_tfms)\n",
    "\n",
    "# def hw2corners(ctr, hw): return torch.cat([ctr-hw/2, ctr+hw/2], dim=1)\n",
    "\n",
    "# anc_grids = [28,14,7,4,2,1]\n",
    "# anc_zooms =  [.7, 2**0, 2**(1/3), 2**(2/3)]\n",
    "# anc_ratios = [(1.,1.), (.5,1.), (1.,.5), (3.,1.), (1.,3.)]\n",
    "\n",
    "# anchor_scales = [(anz*i,anz*j) for anz in anc_zooms for (i,j) in anc_ratios]\n",
    "# k = len(anchor_scales)\n",
    "# anc_offsets = [1/(o*2) for o in anc_grids]\n",
    "# anc_x = np.concatenate([np.repeat(np.linspace(ao, 1-ao, ag), ag)\n",
    "#                         for ao,ag in zip(anc_offsets,anc_grids)])\n",
    "# anc_y = np.concatenate([np.tile(np.linspace(ao, 1-ao, ag), ag)\n",
    "#                         for ao,ag in zip(anc_offsets,anc_grids)])\n",
    "# anc_ctrs = np.repeat(np.stack([anc_x,anc_y], axis=1), k, axis=0)\n",
    "# anc_sizes  =   np.concatenate([np.array([[o/ag,p/ag] for i in range(ag*ag) for o,p in anchor_scales])\n",
    "#                for ag in anc_grids])\n",
    "# grid_sizes = V(np.concatenate([np.array([ 1/ag       for i in range(ag*ag) for o,p in anchor_scales])\n",
    "#                for ag in anc_grids]), requires_grad=False).unsqueeze(1)\n",
    "# anchors = V(np.concatenate([anc_ctrs, anc_sizes], axis=1), requires_grad=False).float()\n",
    "# anchor_cnr = hw2corners(anchors[:,:2], anchors[:,2:])\n",
    "\n",
    "# class StdConv(nn.Module):\n",
    "#     def __init__(self, n_in,n_out,stride=2,dp = 0.1):\n",
    "#         super().__init__()\n",
    "#         self.conv = nn.Conv2d(n_in,n_out,3,stride=stride,padding=1)\n",
    "#         self.bn = nn.BatchNorm2d(n_out)\n",
    "#         self.dropout = nn.Dropout(dp)\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         return self.dropout(self.bn(F.relu(self.conv(x))))\n",
    "    \n",
    "# class OutConv(nn.Module):\n",
    "#     def __init__(self, k, n_in, bias):\n",
    "#         super().__init__()\n",
    "#         self.k = k\n",
    "#         self.oconv1 = nn.Conv2d(n_in, (len(id2cat)+1) * k, 3, padding=1)\n",
    "#         self.oconv2 = nn.Conv2d(n_in, 4 * k, 3, padding = 1)\n",
    "#         self.oconv1.bias.data.zero_().add_(bias)\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         return [flatten_conv(self.oconv1(x), self.k),\n",
    "#                 flatten_conv(self.oconv2(x), self.k)]\n",
    "\n",
    "# def flatten_conv(x,k):\n",
    "#     bs,nf,gx,gy = x.size()\n",
    "#     x = x.permute(0,2,3,1).contiguous()\n",
    "#     return x.view(bs,-1,nf//k)\n",
    "\n",
    "# class SaveFeatures():\n",
    "#     features=None\n",
    "#     def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "#     def hook_fn(self, module, input, output): \n",
    "#         self.features = output\n",
    "#         self.features_in = input\n",
    "#     def remove(self): self.hook.remove()\n",
    "        \n",
    "# cut,lr_cut = model_meta[f_model]\n",
    "# def get_base():\n",
    "#     layers = cut_model(f_model(True), cut)\n",
    "#     return nn.Sequential(*layers)\n",
    "\n",
    "# drop = 0.4\n",
    "\n",
    "# class SSD_Custom4(nn.Module):\n",
    "#     def __init__(self, m_base, k, bias):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.m_base = m_base\n",
    "#         self.sfs = [SaveFeatures(m_base[i]) for i in [5,6]] # 28, 14\n",
    "        \n",
    "#         self.drop = nn.Dropout(drop)\n",
    "#         self.layer2 = StdConv(512,256, dp=drop, stride=1) # 7\n",
    "#         self.layer3 = StdConv(256,256, dp=drop) # 4\n",
    "#         self.layer4 = StdConv(256,256, dp=drop) # 2\n",
    "#         self.layer5 = StdConv(256,256, dp=drop) # 1\n",
    "        \n",
    "#         self.lat6 = nn.Conv2d(256,256,kernel_size=1, stride=1, padding=0)\n",
    "#         self.lat5 = nn.Conv2d(256,256,kernel_size=1, stride=1, padding=0)\n",
    "#         self.lat4 = nn.Conv2d(256,256,kernel_size=1, stride=1, padding=0)\n",
    "#         self.lat3 = nn.Conv2d(256,256,kernel_size=1, stride=1, padding=0)\n",
    "#         self.lat2 = nn.Conv2d(256,256,kernel_size=1, stride=1, padding=0)\n",
    "#         self.lat1 = nn.Conv2d(128,256,kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "#         self.upsamp2 = nn.Upsample(size=(2,2), mode='bilinear')\n",
    "#         self.upsamp4 = nn.Upsample(size=(4,4), mode='bilinear')\n",
    "#         self.upsamp7 = nn.Upsample(size=(7,7), mode='bilinear') # can't use nearest interpol for 4x4 -> 7x7\n",
    "#         self.upsamp14 = nn.Upsample(size=(14,14), mode='bilinear')\n",
    "#         self.upsamp28 = nn.Upsample(size=(28,28), mode='bilinear')\n",
    "        \n",
    "#         self.out1 = OutConv(k, 256, bias)\n",
    "#         self.out2 = OutConv(k, 256, bias)\n",
    "#         self.out3 = OutConv(k, 256, bias)\n",
    "#         self.out4 = OutConv(k, 256, bias)\n",
    "#         self.out5 = OutConv(k, 256, bias)\n",
    "#         self.out6 = OutConv(k, 256, bias)\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         x = self.drop(F.relu(self.m_base(x)))\n",
    "        \n",
    "#         c1 = F.relu(self.sfs[0].features) # 28\n",
    "#         c2 = F.relu(self.sfs[1].features) # 14\n",
    "#         c3 = self.layer2(x) # 7\n",
    "#         c4 = self.layer3(c3) # 4\n",
    "#         c5 = self.layer4(c4) # 2\n",
    "#         c6 = self.layer5(c5) # 1\n",
    "       \n",
    "#         p6 = self.lat6(c6)\n",
    "#         p5 = self.upsamp2(p6) + self.lat5(c5)    \n",
    "#         p4 = self.upsamp4(p5) + self.lat4(c4)\n",
    "#         p3 = self.upsamp7(p4) + self.lat3(c3)\n",
    "#         p2 = self.upsamp14(p3) + self.lat2(c2)\n",
    "#         p1 = self.upsamp28(p2) + self.lat1(c1)\n",
    "        \n",
    "#         o1c,o1l = self.out1(p1)\n",
    "#         o2c,o2l = self.out2(p2)\n",
    "#         o3c,o3l = self.out3(p3)\n",
    "#         o4c,o4l = self.out4(p4)\n",
    "#         o5c,o5l = self.out5(p5)\n",
    "#         o6c,o6l = self.out6(p6)\n",
    "        \n",
    "#         return [torch.cat([o1c,o2c,o3c,o4c,o5c,o6c], dim=1),\n",
    "#                 torch.cat([o1l,o2l,o3l,o4l,o5l,o6l], dim=1)]\n",
    "\n",
    "# class MakeModel():\n",
    "#     def __init__(self,model,name='makemodel'):\n",
    "#         self.model,self.name = model,name\n",
    "\n",
    "#     def get_layer_groups(self, precompute):\n",
    "#         lgs = list(split_by_idxs(children(self.model.m_base), [lr_cut]))\n",
    "#         return lgs + [children(self.model)[1:]]\n",
    "    \n",
    "\n",
    "# class Fake():\n",
    "#     def __init__(self,p):\n",
    "#         self.path = p\n",
    "\n",
    "# def intersection(box_a,box_b):\n",
    "#     min_xy = torch.max(box_a[:,None,:2],box_b[None,:,:2])\n",
    "#     max_xy = torch.min(box_a[:,None,2:],box_b[None,:,2:])\n",
    "#     inter = torch.clamp(max_xy-min_xy,min=0)\n",
    "#     return inter[:,:,0] * inter[:,:,1]\n",
    "\n",
    "# def get_size(box):\n",
    "#     return (box[:,2]-box[:,0]) * (box[:,3] - box[:,1])\n",
    "\n",
    "# def jaccard(box_a,box_b):\n",
    "#     inter = intersection(box_a,box_b)\n",
    "#     union = get_size(box_a).unsqueeze(1) + get_size(box_b).unsqueeze(0) - inter\n",
    "#     return inter/union\n",
    "\n",
    "# #Removes the zero padding in the target bbox/class\n",
    "# def get_y(bbox,clas):\n",
    "#     bbox = bbox.view(-1,4)/size\n",
    "#     bb_keep = ((bbox[:,2] - bbox[:,0])>0.).nonzero()[:,0]\n",
    "#     return bbox[bb_keep], clas[bb_keep]\n",
    "    \n",
    "# def actn_to_bb(actn, anchors):\n",
    "#     actn_bbs = actn\n",
    "#     actn_ctrs = torch.clamp(((actn_bbs[:,:2] * grid_sizes) + anchors[:,:2]),0,size)\n",
    "#     actn_hw = torch.clamp(((1 + actn_bbs[:,2:]) * anchors[:,2:]),0,size)\n",
    "#     return hw2corners(actn_ctrs,actn_hw)\n",
    "\n",
    "# def map_to_ground_truth(overlaps, print_it=False):\n",
    "#     prior_overlap, prior_idx = overlaps.max(1)\n",
    "#     #if print_it: print(prior_overlap)\n",
    "# #     pdb.set_trace()\n",
    "#     gt_overlap, gt_idx = overlaps.max(0)\n",
    "#     gt_overlap[prior_idx] = 1.99\n",
    "#     for i,o in enumerate(prior_idx): gt_idx[o] = i\n",
    "#     return gt_overlap,gt_idx\n",
    "\n",
    "# def nms(boxes, scores, overlap=0.5, top_k=100):\n",
    "#     keep = scores.new(scores.size(0)).zero_().long()\n",
    "#     if boxes.numel() == 0: return keep\n",
    "#     x1 = boxes[:, 0]\n",
    "#     y1 = boxes[:, 1]\n",
    "#     x2 = boxes[:, 2]\n",
    "#     y2 = boxes[:, 3]\n",
    "#     area = torch.mul(x2 - x1, y2 - y1)\n",
    "#     v, idx = scores.sort(0)  # sort in ascending order\n",
    "#     idx = idx[-top_k:]  # indices of the top-k largest vals\n",
    "#     xx1 = boxes.new()\n",
    "#     yy1 = boxes.new()\n",
    "#     xx2 = boxes.new()\n",
    "#     yy2 = boxes.new()\n",
    "#     w = boxes.new()\n",
    "#     h = boxes.new()\n",
    "\n",
    "#     count = 0\n",
    "#     while idx.numel() > 0:\n",
    "#         i = idx[-1]  # index of current largest val\n",
    "#         keep[count] = i\n",
    "#         count += 1\n",
    "#         if idx.size(0) == 1: break\n",
    "#         idx = idx[:-1]  # remove kept element from view\n",
    "#         # load bboxes of next highest vals\n",
    "#         torch.index_select(x1, 0, idx, out=xx1)\n",
    "#         torch.index_select(y1, 0, idx, out=yy1)\n",
    "#         torch.index_select(x2, 0, idx, out=xx2)\n",
    "#         torch.index_select(y2, 0, idx, out=yy2)\n",
    "#         # store element-wise max with next highest score\n",
    "#         xx1 = torch.clamp(xx1, min=x1[i])\n",
    "#         yy1 = torch.clamp(yy1, min=y1[i])\n",
    "#         xx2 = torch.clamp(xx2, max=x2[i])\n",
    "#         yy2 = torch.clamp(yy2, max=y2[i])\n",
    "#         w.resize_as_(xx2)\n",
    "#         h.resize_as_(yy2)\n",
    "#         w = xx2 - xx1\n",
    "#         h = yy2 - yy1\n",
    "#         # check sizes of xx1 and xx2.. after each iteration\n",
    "#         w = torch.clamp(w, min=0.0)\n",
    "#         h = torch.clamp(h, min=0.0)\n",
    "#         inter = w*h\n",
    "#         # IoU = i / (area(a) + area(b) - i)\n",
    "#         rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n",
    "#         union = (rem_areas - inter) + area[i]\n",
    "#         IoU = inter/union  # store result in iou\n",
    "#         # keep only elements with an IoU <= overlap\n",
    "#         idx = idx[IoU.le(overlap)]\n",
    "#     return keep, count\n",
    "\n",
    "# def load_learner():\n",
    "#     export = \"../models/export.pkl\"\n",
    "#     state = torch.load(export)\n",
    "#     id2cat = state.pop(\"classes\")\n",
    "#     cat2id = {c:i for i,c in enumerate(id2cat)}\n",
    "#     p = state.pop(\"path\")\n",
    "#     mm = state.pop(\"model\")\n",
    "#     c = state.pop(\"class\")\n",
    "#     learn1 = c(Fake(Path(\"../../\")/p), mm)\n",
    "#     learn1.load(\"fpn-modanet5\")\n",
    "#     return learn1, id2cat, cat2id\n",
    "\n",
    "# def predict(learner, fn):\n",
    "#     image = open_image(fn)\n",
    "#     image = tfms[1](image, np.zeros(4))[0][None]\n",
    "#     learner.model.cuda()\n",
    "#     learner.model.eval()\n",
    "#     pred_class,pred_bb = learner.model(V(image))\n",
    "    \n",
    "#     a_ic = actn_to_bb(pred_bb[0], anchors)\n",
    "#     clas_pr, clas_ids = pred_class[0].max(1)\n",
    "#     clas_pr = clas_pr.sigmoid()\n",
    "    \n",
    "#     conf_scores = pred_class[0].sigmoid().t().data\n",
    "    \n",
    "#     out1,out2,cc = [],[],[]\n",
    "#     for cl in range(0, len(conf_scores)-1):\n",
    "#         c_mask = conf_scores[cl] > 0.25\n",
    "#         if c_mask.sum() == 0: continue\n",
    "#         scores = conf_scores[cl][c_mask]\n",
    "#         l_mask = c_mask.unsqueeze(1).expand_as(a_ic)\n",
    "#         boxes = a_ic[l_mask].view(-1, 4)\n",
    "#         ids, count = nms(boxes.data, scores, 0.4, 50)\n",
    "#         ids = ids[:count]\n",
    "#         out1.append(scores[ids])\n",
    "#         out2.append(boxes.data[ids])\n",
    "#         cc.append([cl]*count)\n",
    "    \n",
    "#     if not cc:\n",
    "#         print(f\"{i}: empty array\")\n",
    "#         return\n",
    "    \n",
    "#     cc = T(np.concatenate(cc))\n",
    "#     out1 = torch.cat(out1)\n",
    "#     out2 = torch.cat(out2)\n",
    "    \n",
    "#     return out2, cc, out1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
